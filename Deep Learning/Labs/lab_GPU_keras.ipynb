{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b19eea5d-1eca-41ac-a4c8-81c0c9eaa6b1",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980c479-dfb6-45dc-92c4-90ad21830cf6",
   "metadata": {},
   "source": [
    "# **GPU with Keras**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419333e7-5273-4d29-9758-2f6e842e2f50",
   "metadata": {},
   "source": [
    "Estimated time needed: **25** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a99521-c452-461a-a7bc-3127b66979ab",
   "metadata": {},
   "source": [
    "You may have heard of GPUs (Graphics Processing Unit) and CPUs (Central Processing Unit), but what is the difference? GPUs have been commonly seen used by gamers for better visual rendering, but nowadays its applications extend way beyond improving videogame experience. With respect to deep learning, GPUs are extremely helpful by speeding up certain computations. The difference is evident especially for models that train on large datasets, in which the researcher can take advantage of parallel computing to run operations simultaneously and save time. In this lab, you will learn about how to utilize GPU for `tensorflow`, specifically `keras`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e011941-936b-4629-82a0-314bd505afcd",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module6/L1/img_GPU.jpeg\" width=\"600\" alt=\"computer components\">\n",
    "<center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d1315e-2222-46d0-a88a-e487ff723b0f",
   "metadata": {},
   "source": [
    "**_Note_**: Skills Network Labs currently doesn't have any GPUs available. In order to test the difference between CPU and GPU, please run this lab on a local machine or environment that has GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2b2c10-4452-4156-92a9-56c754ca9514",
   "metadata": {},
   "source": [
    "## __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n",
    "            <li><a href=\"#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n",
    "            <li><a href=\"#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Benefits-of-Using-GPU\">Benefits of Using GPU</a>\n",
    "    </li>  \n",
    "    <li>\n",
    "        <a href=\"#Using-CPU\">Using CPU</a>\n",
    "    </li> \n",
    "    <li>\n",
    "        <a href=\"#Using-GPU\">Using GPU</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Check-Availability\">Check Availability</a></li>\n",
    "            <li><a href=\"#Choosing-Specific-GPUs\">Choosing Specific GPUs</a></li>\n",
    "        </ol>    \n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Using-CPU-and-GPU-jointly\">Using CPU and GPU jointly</a>\n",
    "    </li>     \n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28230b5d-8859-45af-8c58-1c0eb8f67a53",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    " - Set environment to CPU/GPU\n",
    " - Control usage of CPU/GPU in parts of the code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dac39e-540d-4162-a182-58eb3ca0f0b3",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56e4b72-4b77-4e7e-9a66-5e12479ce5d3",
   "metadata": {},
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b4f664-9445-4fe6-b0d2-bb3b763bf43a",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6c2dcc-aabb-4cc6-bcea-53e7e4ccacd8",
   "metadata": {},
   "source": [
    "### Installing Required Libraries\n",
    "\n",
    "The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` and before `!pip install --upgrade tensorflow` in the code cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a4448-0613-4e0d-8479-1d1d4483aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d54da88-d1a6-4256-a0e8-e7c6d37e4512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "!pip install --upgrade tensorflow -qqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a38f0bc-72ed-4f60-8115-e99431930772",
   "metadata": {},
   "source": [
    "### Importing Required Libraries\n",
    "\n",
    "_We recommend you import all required libraries in one place, as follows:_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 12:40:49.399708: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-05-21 12:40:49.414007: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747824049.431806 3960802 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747824049.436847 3960802 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1747824049.450413 3960802 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747824049.450434 3960802 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747824049.450436 3960802 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1747824049.450438 3960802 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-05-21 12:40:49.457144: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF 2.19.0 built with CUDA: True\n",
      "Visible GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "import tensorflow as tf, platform, os, ctypes\n",
    "print(\"TF\", tf.__version__, \"built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"Visible GPUs:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7769079-8aa9-409d-bb15-9300a0300293",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 12:50:41.434251: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747824641.446106 3964502 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747824641.449714 3964502 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "# Import the keras library\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.python.client import device_lib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c547c75-bae6-465d-ae3f-c954417704e9",
   "metadata": {},
   "source": [
    "## Benefits of Using GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd56d5b0-e39c-4e2a-8ff8-7530c8ffe962",
   "metadata": {},
   "source": [
    "GPU excels in parallel computing in comparison to CPU. This technique is especially useful for deep learning algorithms, such as building a Convolutional Neural Network (CNN), or as a matter of fact, any neural network. An example of a parallel processing task is performing convolution on an input layer, in which the kernel is multiplied with the input layer matrix, one local region at a time.\n",
    "\n",
    "The runtime difference is especially noticeable when you train a CNN with multiple epochs - tasks where a lot of matrix operations are involved!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10dca2a-434e-434d-bfd0-9ec05274f5af",
   "metadata": {},
   "source": [
    "## Using CPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28b5765-04da-41ca-a4cb-bf091f372d42",
   "metadata": {},
   "source": [
    "By default, `tensorflow` searches for available GPU to use. There are two ways to force your machine to ignore all GPUs and run code with CPU instead.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b26d0c4-ce39-436f-80cd-9f48b1f070b1",
   "metadata": {},
   "source": [
    "If you want the entire code/notebook to run on CPU, you can specify the environment _**before**_ importing tensorflow/keras. If you decide to switch back, you can restart the kernel and run import as usual without the line below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2373f499-2522-4a68-bf48-7e34e791af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the environment variable value\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3babc7-70e9-44f7-aa66-cf02c4337077",
   "metadata": {},
   "source": [
    "If the environment variable `CUDA_VISIBLE_DEVICES` value takes the values 0/1 (or other positive values), the machine is using GPU to run the code. By setting it to -1, it specifies the algorithm to be run with CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c1179aa-b3a6-4201-869f-8538ccdee99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    }
   ],
   "source": [
    "# Check that CPU is used\n",
    "print(os.environ['CUDA_VISIBLE_DEVICES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87d2465-0a91-476d-8089-0682f30243de",
   "metadata": {},
   "source": [
    "If instead you want to use CPU for portions of the code in a notebook, consider the following approach. Here, you specify what to run with `/CPU:0` using a `with` statement. Using `%%timeit` with `-n1 -r1` will time the process for one pass of the cell. As an example, we'll be training the following CNN on a **DATASET**. Feel free to change the code within the statement to test CPU performance! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42182bf5-dc34-406f-9f20-a305cbc9d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99ca5cbd-8e9a-4eb9-af75-c71085816b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data\n",
    "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1],X_train.shape[2],1))\n",
    "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1],X_test.shape[2],1))\n",
    "\n",
    "y_train = y_train.reshape((y_train.shape[0],1))\n",
    "y_test = y_test.reshape((y_test.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26b4f958-3024-41fc-ab36-a8974b8d3e0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-21 12:18:54.915400: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.8802 - loss: 3.3755 - val_accuracy: 0.9584 - val_loss: 0.1954\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9704 - loss: 0.1212 - val_accuracy: 0.9601 - val_loss: 0.1644\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9755 - loss: 0.0889 - val_accuracy: 0.9620 - val_loss: 0.1637\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9791 - loss: 0.0709 - val_accuracy: 0.9666 - val_loss: 0.1369\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.9830 - loss: 0.0605 - val_accuracy: 0.9658 - val_loss: 0.1480\n",
      "1min 1s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "# Building the CNN model and fitting on train data\n",
    "with tf.device('/CPU:0'):\n",
    "    model_cpu = Sequential()\n",
    "    model_cpu.add(Conv2D(input_shape = (28, 28, 1),\n",
    "                     filters=5, \n",
    "                     padding='Same',\n",
    "                     kernel_size=(3,3)\n",
    "                     ))\n",
    "    model_cpu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model_cpu.add(Flatten())\n",
    "    model_cpu.add(Dense(256, activation='relu'))\n",
    "    model_cpu.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model_cpu.compile(optimizer='adam', \n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model_cpu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa8c13a-7343-4adb-889a-4967c8db3eb4",
   "metadata": {},
   "source": [
    "## Using GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8264a9db-9370-48bb-8daa-ca4030667a85",
   "metadata": {},
   "source": [
    "As mentioned above, `tensorflow` automatically searches for GPUs to run on. Let's take a closer look at how we can have more control over that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5aa854-d956-4d3f-91ea-4892e842422f",
   "metadata": {},
   "source": [
    "### Check Availability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab8d633-8fd5-4821-871c-9589b273b003",
   "metadata": {},
   "source": [
    "First, you can check the number of GPUs available on the machine. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaecacc3-cf40-4c63-b4cb-d9e7a8e7c4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2c6ca7-9f19-4396-901e-073eda98a3bf",
   "metadata": {},
   "source": [
    "If you're running this notebook in Skills Network Lab, you can see that it doesn't have any GPUs available for use. However, if your local machine does have GPU(s), you can try the following code to play with what you want to run on GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310da51d-666e-4ab2-a552-d6826358161b",
   "metadata": {},
   "source": [
    "### Choosing Specific GPUs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b427612-282d-487b-bebf-b338bf8b3c8e",
   "metadata": {},
   "source": [
    "In order to specify a particular GPU to run on, we have to first check what units there are in the environment. The following lists out the information of each device, including the device name, type, memory limit, and so on. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4fe61bef-523a-46c2-b07b-3d33fe28c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13567092344815461989\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 82489901056\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13786819334982883431\n",
      "physical_device_desc: \"device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:5f:00.0, compute capability: 9.0\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747824688.735378 3964502 gpu_device.cc:2022] Created device /device:GPU:0 with 78668 MB memory:  -> device: 0, name: NVIDIA H100 80GB HBM3, pci bus id: 0000:5f:00.0, compute capability: 9.0\n"
     ]
    }
   ],
   "source": [
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30395535-d226-4faf-83df-f96e611b9f94",
   "metadata": {},
   "source": [
    "To specify using a specific GPU, again use `tf.device()` with the `name` as input, just like we did for the CPU case. In the `with` statement, proceed with writing code as usual. Here, we are specifying `tensorflow` to be run on GPU ennumerated #2. We also use `%%timeit` here so you can compare the time that GPU took to run in comparison with CPU!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e4b1b29-c754-4fca-bff4-5648d5c689b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1747824693.156468 3964816 service.cc:148] XLA service 0x7bcd20005330 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1747824693.156552 3964816 service.cc:156]   StreamExecutor device (0): NVIDIA H100 80GB HBM3, Compute Capability 9.0\n",
      "I0000 00:00:1747824693.268584 3964816 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  52/1875\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.5535 - loss: 24.1592  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747824695.057166 3964816 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8837 - loss: 3.3655 - val_accuracy: 0.9515 - val_loss: 0.2627\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9697 - loss: 0.1348 - val_accuracy: 0.9559 - val_loss: 0.1993\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9756 - loss: 0.0900 - val_accuracy: 0.9580 - val_loss: 0.1846\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.9799 - loss: 0.0743 - val_accuracy: 0.9629 - val_loss: 0.1471\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9822 - loss: 0.0637 - val_accuracy: 0.9661 - val_loss: 0.1624\n",
      "24 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "\n",
    "with tf.device('/device:GPU:0'):\n",
    "    model_gpu = Sequential()\n",
    "    model_gpu.add(Conv2D(input_shape = (28, 28, 1),\n",
    "                     filters=5, \n",
    "                     padding='Same',\n",
    "                     kernel_size=(3,3)\n",
    "                     ))\n",
    "    model_gpu.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model_gpu.add(Flatten())\n",
    "    model_gpu.add(Dense(256, activation='relu'))\n",
    "    model_gpu.add(Dense(10, activation='softmax'))\n",
    "    \n",
    "    model_gpu.compile(optimizer='adam', \n",
    "              loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    model_gpu.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.18.0\n",
      "Built with CUDA: True\n",
      "Physical GPUs  : [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "print(\"TF version:\", tf.__version__)\n",
    "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
    "print(\"Physical GPUs  :\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2bdc70-fdea-4e6f-9a6f-e72de762d0e4",
   "metadata": {},
   "source": [
    "## Using CPU and GPU jointly\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351769e-b0ee-4935-8460-d88958dcf117",
   "metadata": {},
   "source": [
    "What if we want to use _both_ CPU and GPU for different parts of the same python script? Turns out we can do that too! Simply take advantage of the `tf.device()` function again to specify which unit the code fragment should be run on. Below, we show an example of how to run the same matrix operation on multiple GPUs and add up the tensors on CPU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4a1f7ea-dbf4-4602-b353-139c41da12a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[22. 28.]\n",
      " [49. 64.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Enable tensor allocations or operations to be printed\n",
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "# Get list of all logical GPUs\n",
    "gpus = tf.config.list_logical_devices('GPU')\n",
    "if not gpus:\n",
    "  # Try to set memory growth and re-list GPUs\n",
    "  physical_gpus = tf.config.list_physical_devices('GPU')\n",
    "  if physical_gpus:\n",
    "    for gpu in physical_gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    gpus = tf.config.list_logical_devices('GPU')\n",
    "  else:\n",
    "    print(\"No physical GPUs found by TensorFlow. Check your TensorFlow installation and CUDA/cuDNN setup.\")\n",
    "\n",
    "# Check if there are GPUs on this computer\n",
    "if gpus:\n",
    "  # Run matrix computation on multiple GPUs\n",
    "    c = []\n",
    "    for gpu in gpus:\n",
    "        with tf.device(gpu.name):\n",
    "            a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "            b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]]) \n",
    "            c.append(tf.matmul(a, b))\n",
    "\n",
    "    # Run on CPU \n",
    "    with tf.device('/CPU:0'):\n",
    "        matmul_sum = tf.add_n(c)\n",
    "\n",
    "    print(matmul_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5e08a4-84ed-47a8-be91-94f150505e81",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05568188-b276-45c8-bae8-4830046dad73",
   "metadata": {},
   "source": [
    "[Cindy Huang](https://www.linkedin.com/in/cindy-shih-ting-huang/) is a data science associate of the Skills Network team. She has a passion for machine learning to improve user experience, especially in the area of computational linguistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6163498b-9918-4e3a-9913-6e9d7335d98d",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543eaeaf-3cfb-4a70-bbca-aa68a328f10c",
   "metadata": {},
   "source": [
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/) has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bed1b07-548f-4003-819d-2c41a4232252",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4d9f83-b39e-426b-82c2-bf34ec18a0ea",
   "metadata": {},
   "source": [
    "|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n",
    "|-|-|-|-|\n",
    "|2022-07-11|0.1|Cindy H.|Created Lab|\n",
    "|2022-07-21|0.2|Joseph S.|Reviewed Lab|\n",
    "|2022-08-09|0.3|Steve H.|QA Pass|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307697cd-80ac-4dcf-ad8e-2a96a12651ca",
   "metadata": {},
   "source": [
    "Copyright © 2022 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf215_py311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
